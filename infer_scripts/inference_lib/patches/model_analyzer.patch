diff --git a/model_analyzer/config/input/config_command_profile.py b/model_analyzer/config/input/config_command_profile.py
index 8e05dad..e8dbb6f 100644
--- a/model_analyzer/config/input/config_command_profile.py
+++ b/model_analyzer/config/input/config_command_profile.py
@@ -38,7 +38,7 @@
     DEFAULT_TRITON_HTTP_ENDPOINT, DEFAULT_TRITON_INSTALL_PATH, DEFAULT_TRITON_LAUNCH_MODE, DEFAULT_TRITON_METRICS_URL, \
     DEFAULT_TRITON_SERVER_PATH, DEFAULT_PERF_ANALYZER_TIMEOUT
 
-from model_analyzer.constants import LOGGER_NAME
+from model_analyzer.constants import LOGGER_NAME, PERF_ANALYZER_MEASUREMENT_RELATIVE_STEP
 from model_analyzer.triton.server.server_config import \
     TritonServerConfig
 from model_analyzer.perf_analyzer.perf_config import \
@@ -636,6 +636,14 @@ def _add_perf_analyzer_configs(self):
                 "launched with auto adjusted parameters in an attempt to profile a model. "
             ))
 
+        self._add_config(
+            ConfigField(
+                'perf_analyzer_measurement_relative_step',
+                field_type=ConfigPrimitive(float),
+                default_value=PERF_ANALYZER_MEASUREMENT_RELATIVE_STEP,
+                description="Relative step for stabilization window"
+            ))
+
     def _preprocess_and_verify_arguments(self):
         """
         Enforces some rules on the config.
diff --git a/model_analyzer/constants.py b/model_analyzer/constants.py
index 4920b6f..dabb64c 100644
--- a/model_analyzer/constants.py
+++ b/model_analyzer/constants.py
@@ -32,9 +32,8 @@
 MAX_NUMBER_OF_INTERRUPTS = 3
 
 # Perf Analyzer
-MEASUREMENT_WINDOW_STEP = 1000
-MEASUREMENT_REQUEST_COUNT_STEP = 50
 INTERVAL_SLEEP_TIME = 1
+PERF_ANALYZER_MEASUREMENT_RELATIVE_STEP = 0.5
 PERF_ANALYZER_MEASUREMENT_WINDOW = 5000
 PERF_ANALYZER_MINIMUM_REQUEST_COUNT = 50
 
diff --git a/model_analyzer/model_manager.py b/model_analyzer/model_manager.py
index a5d2044..64bf38b 100644
--- a/model_analyzer/model_manager.py
+++ b/model_analyzer/model_manager.py
@@ -252,7 +252,7 @@ def _create_and_load_model_variant(self, original_name, variant_config):
             except FileExistsError:
                 pass
 
-        if self._config.triton_launch_mode != 'c_api':
+        if self._config.triton_launch_mode not in ['c_api', 'remote']:
             self._client.wait_for_server_ready(self._config.client_max_retries)
 
             if self._client.load_model(model_name=variant_name) == -1:
diff --git a/model_analyzer/perf_analyzer/perf_analyzer.py b/model_analyzer/perf_analyzer/perf_analyzer.py
index af9225b..35e8128 100644
--- a/model_analyzer/perf_analyzer/perf_analyzer.py
+++ b/model_analyzer/perf_analyzer/perf_analyzer.py
@@ -34,8 +34,9 @@
     import PerfServerComputeOutput
 
 from model_analyzer.constants import \
-    INTERVAL_SLEEP_TIME, LOGGER_NAME, MEASUREMENT_REQUEST_COUNT_STEP, \
-    MEASUREMENT_WINDOW_STEP, PERF_ANALYZER_MEASUREMENT_WINDOW, \
+    INTERVAL_SLEEP_TIME, LOGGER_NAME, \
+    PERF_ANALYZER_MEASUREMENT_RELATIVE_STEP, \
+    PERF_ANALYZER_MEASUREMENT_WINDOW, \
     PERF_ANALYZER_MINIMUM_REQUEST_COUNT
 
 from subprocess import Popen, STDOUT, PIPE
@@ -70,7 +71,15 @@ class PerfAnalyzer:
         PerfServerComputeOutput: "_parse_perf_server_compute_output"
     }
 
-    def __init__(self, path, config, max_retries, timeout, max_cpu_util):
+    def __init__(
+            self,
+            path,
+            config,
+            max_retries,
+            timeout,
+            max_cpu_util,
+            measurement_relative_step: int = PERF_ANALYZER_MEASUREMENT_RELATIVE_STEP
+    ):
         """
         Parameters
         ----------
@@ -96,6 +105,19 @@ def __init__(self, path, config, max_retries, timeout, max_cpu_util):
         self._output = None
         self._perf_records = None
         self._max_cpu_util = max_cpu_util
+        self._measurement_relative_step = measurement_relative_step
+
+        self._initial_measurement_interval = int(
+            self._config['measurement-interval']
+            if self._config['measurement-interval'] is not None
+            else PERF_ANALYZER_MEASUREMENT_WINDOW
+        )
+
+        self._initial_measurement_request_count = int(
+            self._config['measurement-request-count']
+            if self._config['measurement-request-count'] is not None
+            else PERF_ANALYZER_MINIMUM_REQUEST_COUNT
+        )
 
     def run(self, metrics, env=None):
         """
@@ -231,24 +253,21 @@ def _auto_adjust_parameters(self, cmd, process):
                                 "Please use a larger time window") != -1:
             if self._config['measurement-mode'] == 'time_windows':
                 if self._config['measurement-interval'] is None:
-                    self._config[
-                        'measurement-interval'] = PERF_ANALYZER_MEASUREMENT_WINDOW + MEASUREMENT_WINDOW_STEP
-                else:
-                    self._config['measurement-interval'] = int(
-                        self._config['measurement-interval']
-                    ) + MEASUREMENT_WINDOW_STEP
+                    self._config['measurement-interval'] = self._initial_measurement_interval
+                self._config['measurement-interval'] = int(self._config['measurement-interval']) + int(
+                    self._initial_measurement_interval * self._measurement_relative_step
+                )
+
                 logger.info(
                     "perf_analyzer's measurement window is too small, "
                     f"increased to {self._config['measurement-interval']} ms.")
             elif self._config['measurement-mode'] is None or self._config[
                     'measurement-mode'] == 'count_windows':
                 if self._config['measurement-request-count'] is None:
-                    self._config[
-                        'measurement-request-count'] = PERF_ANALYZER_MINIMUM_REQUEST_COUNT + MEASUREMENT_REQUEST_COUNT_STEP
-                else:
-                    self._config['measurement-request-count'] = int(
-                        self._config['measurement-request-count']
-                    ) + MEASUREMENT_REQUEST_COUNT_STEP
+                    self._config['measurement-request-count'] = self._initial_measurement_request_count
+                self._config['measurement-request-count'] = int(self._config['measurement-request-count']) + int(
+                    self._initial_measurement_request_count * self._measurement_relative_step
+                )
                 logger.info(
                     "perf_analyzer's request count is too small, "
                     f"increased to {self._config['measurement-request-count']}."
diff --git a/model_analyzer/record/metrics_manager.py b/model_analyzer/record/metrics_manager.py
index e9666c5..93e42ad 100644
--- a/model_analyzer/record/metrics_manager.py
+++ b/model_analyzer/record/metrics_manager.py
@@ -291,7 +291,9 @@ def _get_perf_analyzer_metrics(self,
             config=perf_config,
             max_retries=self._config.perf_analyzer_max_auto_adjusts,
             timeout=self._config.perf_analyzer_timeout,
-            max_cpu_util=self._config.perf_analyzer_cpu_util)
+            max_cpu_util=self._config.perf_analyzer_cpu_util,
+            measurement_relative_step=self._config.perf_analyzer_measurement_relative_step
+        )
 
         # IF running with C_API, need to set CUDA_VISIBLE_DEVICES here
         if self._config.triton_launch_mode == 'c_api':
diff --git a/model_analyzer/record/types/perf_throughput_normalized.py b/model_analyzer/record/types/perf_throughput_normalized.py
new file mode 100644
index 0000000..25311b2
--- /dev/null
+++ b/model_analyzer/record/types/perf_throughput_normalized.py
@@ -0,0 +1,92 @@
+# Copyright (c) 2020-2021, NVIDIA CORPORATION. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from functools import total_ordering
+
+from model_analyzer.record.record import Record
+
+
+@total_ordering
+class PerfThroughputNormalized(Record):
+    """
+    A record for perf_analyzer
+    metric 'Throughput'
+    """
+
+    tag = "perf_throughput_normalized"
+
+    def __init__(self, value, timestamp=0):
+        """
+        Parameters
+        ----------
+        value : float
+            The throughput from the perf analyzer output
+        timestamp : float
+            Elapsed time from start of program
+        """
+
+        super().__init__(value, timestamp)
+
+    @staticmethod
+    def header(aggregation_tag=False):
+        """
+        Parameters
+        ----------
+        aggregation_tag: bool
+            An optional tag that may be displayed 
+            as part of the header indicating that 
+            this record has been aggregated using 
+            max, min or average etc. 
+             
+        Returns
+        -------
+        str
+            The full name of the
+            metric.
+        """
+
+        return "Throughput Per GPU (infer/sec)"
+
+    def __eq__(self, other):
+        """
+        Allows checking for
+        equality between two records
+        """
+
+        return self.value() == other.value()
+
+    def __lt__(self, other):
+        """
+        Allows checking if 
+        this record is less than 
+        the other
+        """
+
+        return self.value() < other.value()
+
+    def __add__(self, other):
+        """
+        Allows adding two records together
+        to produce a brand new record.
+        """
+
+        return PerfThroughputNormalized(value=(self.value() + other.value()))
+
+    def __sub__(self, other):
+        """
+        Allows subtracting two records together
+        to produce a brand new record.
+        """
+
+        return PerfThroughputNormalized(value=(self.value() - other.value()))
diff --git a/model_analyzer/result/result_manager.py b/model_analyzer/result/result_manager.py
index 2677dee..887d008 100644
--- a/model_analyzer/result/result_manager.py
+++ b/model_analyzer/result/result_manager.py
@@ -46,7 +46,8 @@ class ResultManager:
         'instance_group': 'Instance Group',
         'dynamic_batch_sizes': 'Preferred Batch Sizes',
         'satisfies_constraints': 'Satisfies Constraints',
-        'gpu_uuid': 'GPU UUID'
+        'gpu_uuid': 'GPU UUID',
+        "backend_parameters": "Backend Parameters",
     }
 
     server_only_table_key = 'server_gpu_metrics'
@@ -401,6 +402,7 @@ def _tabulate_measurements(self, result):
         model_name = result.model_name()
         instance_group = result.model_config().instance_group_string()
         dynamic_batching = result.model_config().dynamic_batching_string()
+        backend_parameters = result.model_config().backend_parameters_string()
         cpu_only = result.model_config().cpu_only()
 
         passing_measurements = result.passing_measurements()
@@ -415,10 +417,11 @@ def _tabulate_measurements(self, result):
                                            dynamic_batching=dynamic_batching,
                                            measurement=next_best_measurement,
                                            passes=passes,
-                                           cpu_only=cpu_only)
+                                           cpu_only=cpu_only,
+                                           backend_parameters=backend_parameters)
 
-    def _tabulate_measurement(self, model_name, instance_group,
-                              dynamic_batching, measurement, passes, cpu_only):
+    def _tabulate_measurement(self, *, model_name, instance_group,
+                              dynamic_batching, measurement, passes, cpu_only, backend_parameters):
         """
         Add a single measurement to the specified
         table
@@ -436,7 +439,8 @@ def _tabulate_measurement(self, model_name, instance_group,
                                                    concurrency, satisfies,
                                                    model_name, tmp_model_name,
                                                    dynamic_batching,
-                                                   instance_group)
+                                                   instance_group,
+                                                   backend_parameters)
 
         for metric in measurement.non_gpu_data():
             metric_tag_index = self._find_index_for_field(
@@ -456,7 +460,7 @@ def _tabulate_measurement(self, model_name, instance_group,
                                                      concurrency, satisfies,
                                                      model_name, tmp_model_name,
                                                      dynamic_batching,
-                                                     instance_group)
+                                                     instance_group, backend_parameters)
                 gpu_uuid_index = self._find_index_for_field(
                     gpu_fields, 'gpu_uuid')
                 if gpu_uuid_index is not None:
@@ -471,7 +475,7 @@ def _tabulate_measurement(self, model_name, instance_group,
 
     def _get_common_row_items(self, fields, batch_size, concurrency, satisfies,
                               model_name, model_config_path, dynamic_batching,
-                              instance_group):
+                              instance_group, backend_parameters):
         row = [None] * len(fields)
 
         # Model Name
@@ -512,6 +516,12 @@ def _get_common_row_items(self, fields, batch_size, concurrency, satisfies,
                                                         'instance_group')
         if instance_group_idx is not None:
             row[instance_group_idx] = instance_group
+
+        # Backend Parameters
+        parameters_idx = self._find_index_for_field(fields, 'backend_parameters')
+        if parameters_idx is not None:
+            row[parameters_idx] = backend_parameters
+
         return row
 
     def _add_server_data(self):
diff --git a/model_analyzer/state/analyzer_state.py b/model_analyzer/state/analyzer_state.py
index 40a4cef..3127e8b 100644
--- a/model_analyzer/state/analyzer_state.py
+++ b/model_analyzer/state/analyzer_state.py
@@ -11,7 +11,8 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-
+from model_analyzer.record.types.perf_throughput import PerfThroughput
+from model_analyzer.record.types.perf_throughput_normalized import PerfThroughputNormalized
 from model_analyzer.triton.model.model_config import ModelConfig
 from model_analyzer.result.measurement import Measurement
 from model_analyzer.record.record import RecordType
@@ -48,6 +49,7 @@ def from_dict(cls, state_dict):
                 measurements_dict = {}
                 for measurement_key, measurement_dict in measurements.items():
                     measurement = Measurement.from_dict(measurement_dict)
+                    measurement = cls._add_normalize_measurement(measurement, model_config)
                     measurements_dict[measurement_key] = measurement
                 state._state_dict['ResultManager.results'][model_name][
                     model_config_name] = (model_config, measurements_dict)
@@ -78,3 +80,20 @@ def get(self, name):
 
     def set(self, name, value):
         self._state_dict[name] = value
+
+    @classmethod
+    def _add_normalize_measurement(cls, measurement, model_config):
+        throughput_metric = measurement.get_metric(PerfThroughput.tag)
+        model_config = model_config.to_dict()
+        pp = int(model_config["parameters"]["pipeline_para_size"]["stringValue"])
+        tp = int(model_config["parameters"]["tensor_para_size"]["stringValue"])
+        normalized_throughput = throughput_metric.value() / (pp * tp)
+        throughput_normalized_metric = PerfThroughputNormalized(
+            value=normalized_throughput, timestamp=throughput_metric.timestamp()
+        )
+        updated_measurement = Measurement(
+            gpu_data=measurement.gpu_data(),
+            non_gpu_data=measurement.non_gpu_data() + [throughput_normalized_metric],
+            perf_config=measurement.perf_config(),
+        )
+        return updated_measurement
diff --git a/model_analyzer/triton/client/client.py b/model_analyzer/triton/client/client.py
index cc410c1..5c8c12d 100755
--- a/model_analyzer/triton/client/client.py
+++ b/model_analyzer/triton/client/client.py
@@ -165,8 +165,8 @@ def get_model_config(self, model_name, num_retries):
             A dictionary containg the model config.
         """
 
-        self.load_model(model_name)
+        # self.load_model(model_name)
         self.wait_for_model_ready(model_name, num_retries)
         model_config_dict = self._client.get_model_config(model_name)
-        self.unload_model(model_name)
+        # self.unload_model(model_name)
         return model_config_dict
diff --git a/model_analyzer/triton/client/grpc_client.py b/model_analyzer/triton/client/grpc_client.py
index f868a9a..5d29a5b 100755
--- a/model_analyzer/triton/client/grpc_client.py
+++ b/model_analyzer/triton/client/grpc_client.py
@@ -50,9 +50,9 @@ def get_model_config(self, model_name, num_retries):
             A dictionary containg the model config.
         """
 
-        self.load_model(model_name)
+        # self.load_model(model_name)
         self.wait_for_model_ready(model_name, num_retries)
         model_config_dict = self._client.get_model_config(model_name,
                                                           as_json=True)
-        self.unload_model(model_name)
+        # self.unload_model(model_name)
         return model_config_dict['config']
diff --git a/model_analyzer/triton/model/model_config.py b/model_analyzer/triton/model/model_config.py
index 8327561..bf7369e 100644
--- a/model_analyzer/triton/model/model_config.py
+++ b/model_analyzer/triton/model/model_config.py
@@ -282,6 +282,20 @@ def dynamic_batching_string(self):
         else:
             return "Disabled"
 
+    def backend_parameters_string(self):
+        model_config = self.get_config()
+        PARAMS_OF_INTEREST = {"pipeline_para_size": "PP", "tensor_para_size": "TP", "is_half": "half", "max_input_len": "max_input", "max_seq_len": "max_seq"}
+        if 'parameters' in model_config:
+            parameters = model_config["parameters"]
+
+            def _get_entry(k):
+                value = parameters[k]["string_value"]
+                return f"{PARAMS_OF_INTEREST[k]}={value}"
+
+            return f"{' '.join([f'{_get_entry(k)}' for k in PARAMS_OF_INTEREST if k in parameters])}"
+        else:
+            return "Disabled"
+
     def instance_group_string(self):
         """
         Returns

slurm:
  partition: ???
  account: null
  time_limit: "4:00:00"
  nodes: 1
  exclusive: True
  mem: 0
  overcommit: True
  ntasks_per_node: 8
  dependency: "Singleton"
  job_name: "bignlp-gpt3:126m"


run:
  name: "126m"
  blend_path: "prepare_dataset/gpt3_blend.sh"
  log_dir: "train_scripts/logs"
  bind_script: null
  mem_script: null
  cpu_script: null

megatron:
  tensor_model_parallel_size: 1
  pipeline_model_parallel_size: 1
  num_layers: 12
  hidden_size: 768
  num_attention_heads: 12
  seq_length: 2048
  max_position_embeddings: 2048
  micro_batch_size: 4
  global_batch_size: 256
  train_samples: 192000000
  lr_decay_samples: 166400000
  lr_warmup_samples: 162761
  lr: 6.0e-4
  min_lr: 6.0e-5
  lr_decay_style: "cosine"
  log_timers_to_tensorboard: False
  log_interval: 100
  eval_iters: 50
  eval_interval: 2000
  data_path: "$DATA_BLEND"  # Will run the run.blend_path script to get the blend.
  vocab_file: ???
  merge_file: ???
  save_interval: 10000
  save: ???
  load: ???
  split: 9999,1,0
  clip_grad: 1.0
  weight_decay: 0.1
  adam_beta1: 0.9
  adam_beta2: 0.95
  init_method_std: 0.023
  log_params_norm: True
  log_num_zeros_in_grad: True
  fp16: True
  DDP_impl: "torch"
  tensorboard_dir: ???

slurm:
  partition: ???
  account: null
  time_limit: "4:00:00"
  nodes: 20
  exclusive: True
  mem: 0
  overcommit: True
  ntasks_per_node: 8
  dependency: "Singleton"
  job_name: "bignlp-gpt3:5b"


run:
  name: "5b"
  container: "nvcr.io#nvidia/pytorch:20.12-py3"
  blend_path: "prepare_dataset/gpt3_blend.sh"
  log_dir: "train_scripts/logs"
  bind_script: null
  mem_script: null
  cpu_script: null

megatron:
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  num_layers: 24
  hidden_size: 4096
  num_attention_heads: 32
  seq_length: 2048
  max_position_embeddings: 2048
  micro_batch_size: 4
  global_batch_size: 1280
  train_samples: 160000000
  lr_decay_samples: 166400000
  lr_warmup_samples: 244141
  lr: 1.2e-4
  min_lr: 1.2e-5
  lr_decay_style: "cosine"
  log_timers_to_tensorboard: False
  log_interval: 100
  eval_iters: 50
  eval_interval: 2000
  data_path: "$DATA_BLEND"  # Will run the run.blend_path script to get the blend.
  vocab_file: ???
  merge_file: ???
  save_interval: 10000
  save: ???
  load: ???
  split: 9999,1,0
  clip_grad: 1.0
  weight_decay: 0.1
  adam_beta1: 0.9
  adam_beta2: 0.95
  init_method_std: 0.023
  log_params_norm: True
  log_num_zeros_in_grad: True
  fp16: True
  DDP_impl: "torch"
  tensorboard_dir: ???

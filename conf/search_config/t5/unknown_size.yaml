train_settings:
  model_size_in_b: null # unit in billion parameters
  num_nodes: 80
  gpus_per_node: 8
  gpu_memory_gb: 80  # Memory per GPU, in GB. Currently 40GB and 80GB A100s supported.
  max_training_days: 50 # unit in days
  limit_search_runs: 100 # Max number of runs to be launched in parallel for grid search.
  output_top_n: 10  # The result will print the top N fastest training configs.
  max_steps_per_run: 50 # Max steps per run for the grid search.
  max_minutes_per_run: 20 # minutes per run for the grid search.
  tflops_per_gpu: 140  # Estimated tflops per GPU.
  num_tokens_in_b: 1000  # Unit in billions, typically 300B for GPT3 models.
  vocab_size: 29000
  logs: ${base_results_dir}/${search_config_value}_${.gpu_memory_gb}gb  # Example base_results_dir/t5/0.22b
  override_search_num_nodes: auto  # auto to use the minimum required number of nodes
  tensor_parallel_sizes: auto  # auto to use our recommendation, or a list, such as [1, 2, 4, 8]
  pipeline_parallel_sizes: auto  # auto to use our recommendation, or a list, such as [1, 2, 4, 8, 10]
  micro_batch_sizes: auto  # auto to use our recommendation, or a list, such as [1, 2, 4, 8, 16]
  act_ckpt_layers: auto  # auto to use our recommendation, or a list, such as [0, 1, 2, 3]
 
inference_settings:
  run:
    model_type: t5
    model_train_name: t5_unknown
    data_type: fp16 # fp32|fp16|bf16
    time_limit: 0:30:00
    results_dir: ${base_results_dir}/${search_config_value}_${...train_settings.gpu_memory_gb}gb
    top_n: 10
    max_latency_ms: 500
    tensor_parallel_sizes: [1,2]
    pipeline_parallel_sizes: [1]
  benchmark:
    input_len: 60
    output_len: 20
    batch_sizes: [8, 16, 32, 64, 128, 256]
    triton_wait_time_s: 300
    vocab_size: 29184


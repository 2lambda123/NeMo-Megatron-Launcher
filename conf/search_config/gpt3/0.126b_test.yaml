train_settings:
  model_size_in_b: 0.126
  num_nodes: 8
  gpus_per_node: 8
  gpu_memory_gb: 80
  max_training_days: 2
  limit_search_runs: 100
  output_top_n: 10
  max_steps_per_run: 50
  max_minutes_per_run: 20
  tflops_per_gpu: 140
  num_tokens_in_b: 300
  vocab_size: 51200
  logs: ${base_results_dir}/${search_config_value}_${.gpu_memory_gb}gb
  tensor_parallel_sizes: auto
  pipeline_parallel_sizes: auto
  micro_batch_sizes: auto
  act_ckpt_layers: auto

inference_settings:
  run:
    model_type: gpt3
    model_train_name: gpt3_0.126b
    data_type: fp16
    timelimit: 0:30:00
    results_dir: ${base_results_dir}/${search_config_value}
    top_n: 10
    max_latency_ms: 500
    tensor_parallel_sizes: [1, 2]
    pipeline_parallel_sizes: [1]
  benchmark:
    input_len: 60
    output_len: 20
    batch_sizes: [8, 16, 32, 64, 128, 256]
    triton_wait_time_s: 300


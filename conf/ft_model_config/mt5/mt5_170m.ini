[encoder]
relative_attention_num_buckets_or_max_pos_seq_len = 32
num_layers = 8
d_model = 512
d_ff = 1024
num_heads = 6
d_kv = 64
vocab_size = 250112
_name_or_path = 170m
weight_data_type = fp32
feed_forward_proj = relu
model_type = T5

[decoder]
relative_attention_num_buckets_or_max_pos_seq_len = 32
num_layers = 8
d_model = 512
d_ff = 1024
num_heads = 6
d_kv = 64
vocab_size = 250112
decoder_start_token_id = 0
eos_token_id = 1
_name_or_path = 170m
weight_data_type = fp32
model_type = T5

[structure]
t5_with_bias = 1
use_gated_activation = 1
position_embedding_type = absolute

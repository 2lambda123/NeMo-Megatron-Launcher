[encoder]
relative_attention_num_buckets_or_max_pos_seq_len = 32
num_layers = 36
d_model = 5120
d_ff = 10880
num_heads = 80
d_kv = 64
vocab_size = 29184
_name_or_path = 23b
weight_data_type = fp32

[decoder]
relative_attention_num_buckets_or_max_pos_seq_len = 32
num_layers = 36
d_model = 5120
d_ff = 10880
num_heads = 80
d_kv = 64
vocab_size = 29184
decoder_start_token_id = 0
eos_token_id = 1
_name_or_path = 23b
weight_data_type = fp32

[structure]
t5_with_bias = 1
use_gated_activation = 0
position_embedding_type = absolute

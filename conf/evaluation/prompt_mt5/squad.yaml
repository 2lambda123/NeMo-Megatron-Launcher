run:
  name: eval_${.task_name}_${.model_train_name}
  time_limit: "04:00:00"
  dependency: "singleton"
  model_train_name: mt5_390m
  task_name: "squad"  # SQuAD v1.1
  prompt_learning_dir: ${base_results_dir}/${.model_train_name}/prompt_learning_squad # assume prompt learning was on squad task
  results_dir: ${base_results_dir}/${.model_train_name}/${.task_name}_eval

trainer:
  devices: ${divide_ceil:${evaluation.model_parallel_size}, ${.num_nodes}}
  num_nodes: ${divide_ceil:${evaluation.model_parallel_size}, 8}
  accelerator: gpu
  precision: bf16
  logger: False # logger provided by exp_manager
  enable_checkpointing: False
  replace_sampler_ddp: False
  log_every_n_steps: 10

data:
  test_ds:
    - ${data_dir}/prompt_data/squad-v2.0/squad_test.jsonl
  num_workers: 4
  global_batch_size: 16
  micro_batch_size: 16

tensor_model_parallel_size: 1
pipeline_model_parallel_size: 1
pipeline_model_parallel_split_rank: 0 # used for encoder and decoder model
model_parallel_size: ${multiply:${.tensor_model_parallel_size}, ${.pipeline_model_parallel_size}}
pretrained_language_model_file: ${base_results_dir}/${evaluation.run.model_train_name}/convert_nemo/results/megatron_mt5.nemo  # path to a pretrained mt5 nemo file
virtual_prompt_model_file: ${evaluation.run.prompt_learning_dir}/results/megatron_mt5_prompt.nemo # path to a Megatronmt5PromptLearningModel nemo file
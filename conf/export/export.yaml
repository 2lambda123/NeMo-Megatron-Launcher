run:
  name: export_${training.run.name}
  time_limit: "2:00:00"
  results_dir: ${training.run.results_dir}/export
  triton_model_dir: ${.results_dir}/model_repo/${training.run.name}

conversion:
  checkpoint_path: ${training.run.results_dir}/checkpoints
  # FT checkpoint will be saved in ${.triton_model_dir}/1/${.tensor_model_parallel_size}-gpu
  tensor_model_parallel_size: 8
  weight_data_type: fp16   # fp32|fp16
  processes: 16
  load_checkpoints_to_cpu: False

triton_deployment:
  max_batch_size: 1
  pipeline_model_parallel_size: 1
  int8_mode: False
  enable_custom_all_reduce: False
  data_type: fp16  # fp32|fp16|bf16

accuracy:
  ntasks_per_node: 8  # usually should be number of available gpus per node
  runtime_config_ini_path: ${export.run.results_dir}/ft_runtime.ini
  lambada_path: ${export.run.results_dir}/lambada_test.jsonl
  output_path: ${export.run.results_dir}/lambada_output.json
  batch_size: 64
  runtime:
    max_seq_len: 512
    beam_width: 1
    sampling_top_k: 1
    sampling_top_p: 0
    data_type: ${export.triton_deployment.data_type}  # fp32|fp16|bf16
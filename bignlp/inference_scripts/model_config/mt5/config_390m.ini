[encoder]
relative_attention_num_buckets_or_max_pos_seq_len = 32
num_layers = 12
d_model = 768
d_ff = 2048
num_heads = 12
d_kv = 64
vocab_size = 29184
_name_or_path = 390m
weight_data_type = fp32

[decoder]
relative_attention_num_buckets_or_max_pos_seq_len = 32
num_layers = 12
d_model = 768
d_ff = 2048
num_heads = 12
d_kv = 64
vocab_size = 29184
decoder_start_token_id = 0
eos_token_id = 1
_name_or_path = 390m
weight_data_type = fp32

[structure]
t5_with_bias = 1
use_gated_activation = 1
position_embedding_type = absolute

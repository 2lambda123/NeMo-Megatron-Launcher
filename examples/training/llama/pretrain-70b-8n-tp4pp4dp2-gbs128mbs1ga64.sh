NEMO_MEGATRON_LAUNCHER_DIR=
TOKENIZER=
CONTAINER=
ACCOUNT=

HYDRA_FULL_ERROR=1 python3 ${NEMO_MEGATRON_LAUNCHER_DIR}/launcher_scripts/main.py \
    training=llama/llama2_70b \
    stages=["training"] \
    launcher_scripts_path=${NEMO_MEGATRON_LAUNCHER_DIR}/launcher_scripts \
    base_results_dir=${NEMO_MEGATRON_LAUNCHER_DIR}/results \
    data_dir=${NEMO_MEGATRON_LAUNCHER_DIR} \
    container_mounts=[${TOKENIZER}] \
    container=/lustre/fsw/coreai_dlalgo_llm/chcui/nemo10206771.sqsh \
    cluster.partition=batch \
    cluster.account=${ACCOUNT} \
    cluster.gpus_per_task=null \
    cluster.gpus_per_node=null \
    cluster.job_name_prefix="${ACCOUNT}:llama_70b:" \
    training.exp_manager.create_checkpoint_callback=False \
    +training.exp_manager.log_global_rank_0_only=True \
    +training.exp_manager.create_tensorboard_logger=False \
    training.run.name="70b-8n-tp4pp4dp2-gbs128mbs1ga32" \
    training.run.time_limit=00:10:00 \
    training.trainer.max_time=00:04:00:00 \
    training.trainer.num_nodes=8 \
    training.trainer.devices=8 \
    +training.trainer.enable_model_summary=False \
    +training.trainer.num_sanity_val_steps=0 \
    +training.trainer.benchmark=False \
    training.trainer.log_every_n_steps=1 \
    training.model.data.data_impl=mock \
    training.model.data.data_prefix=[] \
    training.model.tokenizer.model=${TOKENIZER} \
    training.model.global_batch_size=128 \
    training.model.mcore_gpt=True \
    training.model.transformer_engine=False \
    training.model.tensor_model_parallel_size=4 \
    training.model.pipeline_model_parallel_size=4 \
    training.model.fp8=True \
    training.model.fp8_e4m3=False \
    training.model.fp8_hybrid=True \
    training.model.fp8_margin=0 \
    training.model.fp8_interval=1 \
    training.model.fp8_amax_history_len=1024 \
    training.model.fp8_amax_compute_algo=max \
    training.model.encoder_seq_length=2048 \
    training.model.max_position_embeddings=2048 \
    training.model.data.seq_length=2048 \
    +training.model.fp8_wgrad=True \
    +training.model.optim.grad_sync_dtype=bf16 \

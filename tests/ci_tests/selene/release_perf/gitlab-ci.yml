variables: &VARS
  BIGNLP_CI_PATH: "/lustre/fsw/joc/big_nlp/bignlp_ci"
  MEMORY_MEASURE_TEST: "False" # Set to true if you want to run memory test

stages:
  - build
  - gpt3_configs
  - t5_configs
  - mt5_configs
  - cleanup

################
# JOB Templates
################

before_script:
  - umask 0007
  - export DOCKERFILE=./Dockerfile
  - export FROM_IMAGE_NAME=$BIGNLP_BASE_IMAGE
  - export IMAGE_NAME=bignlp_ci
  - export BUILD_IMAGE_NAME=${BIGNLP_REGISTRY}/${IMAGE_NAME}:pipe.${CI_PIPELINE_ID}
  - export BUILD_IMAGE_NAME_SRUN="${BUILD_IMAGE_NAME/:5005\//#}"
  - export PIPELINE_DIR="${BIGNLP_CI_PATH}/${CI_PIPELINE_ID}"
  - export BASE_RESULTS_DIR="${PIPELINE_DIR}/results"

.build: &build_template
  stage: build
  script:
    - docker login -u gitlab-ci-token -p $CI_JOB_TOKEN "${CI_REGISTRY}"
    - export DOCKER_REGISTRY="${CI_REGISTRY/:5005/}"
    - docker login -u gitlab-ci-token -p $CI_JOB_TOKEN "${DOCKER_REGISTRY}"
    - docker login -u "\$oauthtoken" -p $NGC_CLI_API_KEY nvcr.io
    # Define bignlp build image
    - set -x
    - ls
    - env
    - export FROM_IMAGE_ARG="--build-arg FROM_IMAGE_NAME=${FROM_IMAGE_NAME}"
    - docker build -t ${BUILD_IMAGE_NAME} ${FROM_IMAGE_ARG} .
    - docker push ${BUILD_IMAGE_NAME}
  allow_failure: false
  tags:
    - vm-builder

.LUNA: &LUNA
  variables: &LUNA_VARS
    SLURM_PARTITION: "luna"
    SLURM_NIGHTLY_PARTITION: "luna"
    SLURM_ACCOUNT: "joc"
    CLUSTER:       "selene"
    PYXIS_LITE:    "1"
    ENROOT_MOUNT_HOME: "n"
    GIT_CLONE_PATH: $CI_BUILDS_DIR/$SLURM_ACCOUNT/big_nlp/bignlp_ci/$CI_PIPELINE_ID/$CI_JOB_ID/$CI_PROJECT_NAME # THIS DOES NOT HAVE QUOTES FOR A REASON
    EXCLUDE_NODES: ""
    GPU_ARCH:      "A100"
  tags: &LUNA_TAGS
    - selene_ssh

build-BigNLP:
  <<: *build_template
  rules:
    - when: always


.bignlp-LUNA-test-LAUNCHER: &bignlp-LUNA-test-LAUNCHER
  tags: *LUNA_TAGS
  script: &bignlp-LUNA-test-LAUNCHER-SCRIPT
    - chmod 774 ./* -R
    - umask 0007
    - source /lustre/fsw/joc/big_nlp/nemo_gpt3/my_venv/bin/activate
    - set -x
    - export PIPELINE_DIR="${SELENE_BIGNLP_CI_PATH}/${CI_PIPELINE_ID}"
    - export BASE_RESULTS_DIR="${PIPELINE_DIR}/results"
    # Set up a working directory that is available cluster-wide.
    - export CONFIG_DIR="${SELENE_BIGNLP_CI_PATH}/.config/${CI_JOB_ID}" && echo "CONFIG_DIR=${CONFIG_DIR}"
    - mkdir -p "${CONFIG_DIR}" && chmod 755 "${CONFIG_DIR}"
    # Emulate "enroot login" since enroot is not available on the head node
    - export ENROOT_CONFIG_PATH="${CONFIG_DIR}/.enroot" && mkdir -p "${ENROOT_CONFIG_PATH}"
    - echo "machine ${CI_REGISTRY/:5005/} login gitlab-ci-token password ${CI_BUILD_TOKEN}"
    - echo "machine ${CI_REGISTRY/:5005/} login gitlab-ci-token password ${CI_BUILD_TOKEN}" > "${ENROOT_CONFIG_PATH}/.credentials"
    - env
    - source tests/ci_tests/selene/release_perf/scripts/${RUN_MODEL}/train_${RUN_MODEL}_model.sh
    - echo "Job submitted"
    # Wait for job to launch
    - sleep 10s # Without this, "sacct" in jobstate.sh does not always find the SLURM job.
    - export SLURM_JOBID=$(grep 'Submitted batch job' "${RESULTS_DIR}/launcher.log" | awk '{ print $4 }')
    - echo $SLURM_JOBID
    - export SLURM_OUTPUT=${RESULTS_DIR}/slurm_${SLURM_JOBID}.out
    # export SLURM_OUTPUT=$(scontrol show job "${SLURM_JOBID}" | grep 'StdOut' | awk -F '=' '{ print $2 }')
    - cd tests/ci_tests/utils
    - chmod 777 ./* -R
    - bash jobwait.sh "${SLURM_JOBID}" & PID=$!
    - touch "${SLURM_OUTPUT}"
    - \[ ! -z ${SLURM_JOBID} \] && echo -e " --------------------------------------------------\n"
                "----------WAITING FOR SLURM JOB TO BEGIN-----------\n"
                "---------------------------------------------------\n"
                "$(scontrol show job=${SLURM_JOBID})\n"
                "---------------------------------------------------\n"
    # Gitlab logs collapsible section markers
    - echo -e "\e[0Ksection_end:`date +%s`:slurm_setup\r\e[0K"
    # Follow output of the job
    - tail --pid="${PID}" -f "${SLURM_OUTPUT}" # Stream job output until it finishes.
    - echo "Finished job with name ${RUN_NAME}"
    - export SLURM_NODELIST=$(./jobnodes.sh "${SLURM_JOBID}");    echo "SLURM_NODELIST='${SLURM_NODELIST}'"
    - export SLURM_STATE=$(./jobstate.sh "${SLURM_JOBID}");       echo "SLURM_JOBID=${SLURM_JOBID} SLURM_STATE='${SLURM_STATE}'"
    - export SLURM_WALLTIME=$(./jobtime.sh "${SLURM_JOBID}");     echo "SLURM_WALLTIME=${SLURM_WALLTIME} secs"
    - export SLURM_EXITCODE=$(./jobexitcode.sh "${SLURM_JOBID}" || echo $?); echo "SLURM_EXITCODE='${SLURM_EXITCODE}'"
    - cd ${GIT_CLONE_PATH}
    # Run metrics collection
    - git clone ssh://git@gitlab-master.nvidia.com:12051/dl/JoC/bignlp-scripts.git
    - cd bignlp-scripts
    - pushd public/data/${RUN_MODEL}
    - git checkout -b $CI_PIPELINE_ID
    - GIT_BRANCHES=`git branch -r`
    - if [[ "$GIT_BRANCHES" =~ .*"$CI_PIPELINE_ID".* ]]; then git pull origin $CI_PIPELINE_ID; fi
    - GIT_RESULTS_DIR_PATH=`pwd`
    - popd
    - |
        if [[ "$MEMORY_MEASURE_TEST" == "True" ]]; then 
          python3 tests/ci_tests/utils/get_memory_metrics.py $RESULTS_DIR $GIT_RESULTS_DIR_PATH $BIGNLP_BASE_IMAGE
        else 
          python3 tests/ci_tests/utils/get_train_time.py $RESULTS_DIR $GIT_RESULTS_DIR_PATH $BIGNLP_BASE_IMAGE; 
        fi
    # Replace first occurance of tests_to_run_on_this_commit with none. 
    - | 
      sed -i '0,/TESTS_TO_RUN_ON_THIS_COMMIT: .*/s/TESTS_TO_RUN_ON_THIS_COMMIT: .*/TESTS_TO_RUN_ON_THIS_COMMIT: NONE/' .gitlab-ci.yml
    - git add -A
    - git commit -m "Adding results file for $RUN_MODEL"
    - git push origin $CI_PIPELINE_ID
  allow_failure: false
  rules:
    - when: manual
  needs:
    - build-BigNLP

#### Release Configs ####
## GPT3
train.gpt3.126m_2node_100steps:
  stage: gpt3_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 126m
    NUM_NODES: 2
    TIME_LIMIT: "20:00"
    MEMORY_MEASURE_TEST: "True"

train.gpt3.126m_8node_100steps:
  stage: gpt3_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 126m
    NUM_NODES: 8
    TIME_LIMIT: "20:00"

train.gpt3.5b_5node_100steps:
  stage: gpt3_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 5b
    NUM_NODES: 4
    TIME_LIMIT: "4:00:00"

train.gpt3.5b_20node_100steps:
  stage: gpt3_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 5b
    NUM_NODES: 16
    TIME_LIMIT: "4:00:00"

train.gpt3.20b_20node_100steps:
  stage: gpt3_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 20b
    NUM_NODES: 16
    TIME_LIMIT: "3:00:00"

train.gpt3.20b_80node_100steps:
  stage: gpt3_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 20b
    NUM_NODES: 64
    TIME_LIMIT: "3:00:00"

train.gpt3.40b_20node_100steps:
  stage: gpt3_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 40b
    NUM_NODES: 32
    TIME_LIMIT: "3:00:00"


train.gpt3.40b_80node_100steps:
  stage: gpt3_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 40b
    NUM_NODES: 128
    TIME_LIMIT: "3:00:00"

train.gpt3.175b_32node_50steps:
  stage: gpt3_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 175b
    MAX_STEPS: 50
    NUM_NODES: 32
    TIME_LIMIT: "4:00:00"

train.gpt3.175b_128node_50steps:
  stage: gpt3_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 175b
    MAX_STEPS: 50
    NUM_NODES: 128
    TIME_LIMIT: "4:00:00"

## T5
train.t5.220m_1node_100steps:
  stage: t5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 220m
    NUM_NODES: 1
    TIME_LIMIT: "20:00"

train.t5.220m_4node_100steps:
  stage: t5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 220m
    NUM_NODES: 4
    TIME_LIMIT: "20:00"

train.t5.3b_5node_100steps:
  stage: t5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 5
    TIME_LIMIT: "3:00:00"

train.t5.3b_20node_100steps:
  stage: t5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 20
    TIME_LIMIT: "3:00:00"

train.t5.11b_5node_100steps:
  stage: t5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 11b
    NUM_NODES: 5
    TIME_LIMIT: "3:00:00"

train.t5.11b_20node_100steps:
  stage: t5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 11b
    NUM_NODES: 20
    TIME_LIMIT: "3:00:00"

train.t5.23b_10node_100steps:
  stage: t5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 23b
    NUM_NODES: 10
    TIME_LIMIT: "3:00:00"

train.t5.23b_40node_100steps:
  stage: t5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 23b
    NUM_NODES: 40
    TIME_LIMIT: "3:00:00"

train.t5.41b_10node_100steps:
  stage: t5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 41b
    NUM_NODES: 10
    TIME_LIMIT: "3:00:00"

train.t5.41b_40node_100steps:
  stage: t5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 41b
    NUM_NODES: 40
    TIME_LIMIT: "3:00:00"

# mT5
train.mt5.170m_1node_100steps:
  stage: mt5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 170m
    NUM_NODES: 1
    TIME_LIMIT: "1:00:00"



train.mt5.170m_4node_100steps:
  stage: mt5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 170m
    NUM_NODES: 4
    TIME_LIMIT: "1:00:00"

train.mt5.390m_2node_100steps:
  stage: mt5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 390m
    NUM_NODES: 2
    TIME_LIMIT: "1:00:00"

train.mt5.390m_8node_100steps:
  stage: mt5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 390m
    NUM_NODES: 8
    TIME_LIMIT: "1:00:00"

train.mt5.3b_5node_100steps:
  stage: mt5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 5
    TIME_LIMIT: "3:00:00"

train.mt5.3b_20node_100steps:
  stage: mt5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 20
    TIME_LIMIT: "3:00:00"

train.mt5.11b_5node_100steps:
  stage: mt5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 11b
    NUM_NODES: 5
    TIME_LIMIT: "3:00:00"

train.mt5.11b_20node_100steps:
  stage: mt5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 11b
    NUM_NODES: 20
    TIME_LIMIT: "3:00:00"

train.mt5.23b_10node_100steps:
  stage: mt5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 23b
    NUM_NODES: 10
    TIME_LIMIT: "3:00:00"

train.mt5.23b_40node_100steps:
  stage: mt5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 23b
    NUM_NODES: 40
    TIME_LIMIT: "3:00:00"


#### TP PP Comparison ####
## GPT3
# O1
train.gpt3.5b_tp2_pp1_1node_fp16_O1_100steps:
  stage: gpt3_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 5b
    NUM_NODES: 1
    PRECISION: 16
    TP_SIZE: 2
    PP_SIZE: 1
    TIME_LIMIT: "4:00:00"
    AMP_STYLE: O1

train.gpt3.5b_tp4_pp1_1node_fp16_O1_100steps:
  stage: gpt3_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 5b
    NUM_NODES: 1
    PRECISION: 16
    TP_SIZE: 4
    PP_SIZE: 1
    TIME_LIMIT: "4:00:00"
    AMP_STYLE: O1

train.gpt3.5b_tp8_pp1_1node_fp16_O1_100steps:
  stage: gpt3_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 5b
    NUM_NODES: 1
    PRECISION: 16
    TP_SIZE: 8
    PP_SIZE: 1
    TIME_LIMIT: "4:00:00"
    AMP_STYLE: O1

train.gpt3.5b_tp2_pp1_1node_bf16_O1_100steps:
  stage: gpt3_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 5b
    NUM_NODES: 1
    PRECISION: bf16
    TP_SIZE: 2
    PP_SIZE: 1
    TIME_LIMIT: "4:00:00"
    AMP_STYLE: O1

train.gpt3.5b_tp4_pp1_1node_bf16_O1_100steps:
  stage: gpt3_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 5b
    NUM_NODES: 1
    PRECISION: bf16
    TP_SIZE: 4
    PP_SIZE: 1
    TIME_LIMIT: "4:00:00"
    AMP_STYLE: O1

train.gpt3.5b_tp8_pp1_1node_bf16_O1_100steps:
  stage: gpt3_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 5b
    NUM_NODES: 1
    PRECISION: bf16
    TP_SIZE: 8
    PP_SIZE: 1
    TIME_LIMIT: "4:00:00"
    AMP_STYLE: O1

train.gpt3.5b_tp2_pp1_4node_fp16_O1_100steps:
  stage: gpt3_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 5b
    NUM_NODES: 4
    PRECISION: 16
    TP_SIZE: 2
    PP_SIZE: 1
    TIME_LIMIT: "4:00:00"
    AMP_STYLE: O1

train.gpt3.5b_tp4_pp1_4node_fp16_O1_100steps:
  stage: gpt3_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 5b
    NUM_NODES: 4
    PRECISION: 16
    TP_SIZE: 4
    PP_SIZE: 1
    TIME_LIMIT: "4:00:00"
    AMP_STYLE: O1

train.gpt3.5b_tp8_pp1_4node_fp16_O1_100steps:
  stage: gpt3_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 5b
    NUM_NODES: 4
    PRECISION: 16
    TP_SIZE: 8
    PP_SIZE: 1
    TIME_LIMIT: "4:00:00"
    AMP_STYLE: O1

train.gpt3.5b_tp2_pp1_4node_bf16_O1_100steps:
  stage: gpt3_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 5b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 2
    PP_SIZE: 1
    TIME_LIMIT: "4:00:00"
    AMP_STYLE: O1

train.gpt3.5b_tp4_pp1_4node_bf16_O1_100steps:
  stage: gpt3_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 5b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 4
    PP_SIZE: 1
    TIME_LIMIT: "4:00:00"
    AMP_STYLE: O1

train.gpt3.5b_tp8_pp1_4node_bf16_O1_100steps:
  stage: gpt3_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 5b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 8
    PP_SIZE: 1
    TIME_LIMIT: "4:00:00"
    AMP_STYLE: O1

train.gpt3.5b_tp2_pp2_4node_bf16_O1_100steps:
  stage: gpt3_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 5b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 2
    PP_SIZE: 2
    TIME_LIMIT: "4:00:00"
    AMP_STYLE: O1

train.gpt3.5b_tp4_pp2_4node_bf16_O1_100steps:
  stage: gpt3_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 5b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 4
    PP_SIZE: 2
    TIME_LIMIT: "4:00:00"
    AMP_STYLE: O1

train.gpt3.5b_tp8_pp2_4node_bf16_O1_100steps:
  stage: gpt3_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 5b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 8
    PP_SIZE: 2
    TIME_LIMIT: "4:00:00"
    AMP_STYLE: O1

train.gpt3.5b_tp2_pp4_4node_bf16_O1_100steps:
  stage: gpt3_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 5b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 2
    PP_SIZE: 2
    TIME_LIMIT: "4:00:00"
    AMP_STYLE: O1

train.gpt3.5b_tp4_pp4_4node_bf16_O1_100steps:
  stage: gpt3_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 5b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 4
    PP_SIZE: 4
    TIME_LIMIT: "4:00:00"
    AMP_STYLE: O1

train.gpt3.5b_tp8_pp4_4node_bf16_O1_100steps:
  stage: gpt3_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 5b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 8
    PP_SIZE: 4
    TIME_LIMIT: "4:00:00"
    AMP_STYLE: O1

# O2
train.gpt3.5b_tp2_pp1_1node_bf16_O2_100steps:
  stage: gpt3_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 5b
    NUM_NODES: 1
    PRECISION: bf16
    TP_SIZE: 2
    PP_SIZE: 1
    TIME_LIMIT: "4:00:00"
    AMP_STYLE: O2

train.gpt3.5b_tp4_pp1_1node_bf16_O2_100steps:
  stage: gpt3_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 5b
    NUM_NODES: 1
    PRECISION: bf16
    TP_SIZE: 4
    PP_SIZE: 1
    TIME_LIMIT: "4:00:00"
    AMP_STYLE: O2

train.gpt3.5b_tp8_pp1_1node_bf16_O2_100steps:
  stage: gpt3_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 5b
    NUM_NODES: 1
    PRECISION: bf16
    TP_SIZE: 8
    PP_SIZE: 1
    TIME_LIMIT: "4:00:00"
    AMP_STYLE: O2

train.gpt3.5b_tp2_pp1_4node_bf16_O2_100steps:
  stage: gpt3_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 5b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 2
    PP_SIZE: 1
    TIME_LIMIT: "4:00:00"
    AMP_STYLE: O2

train.gpt3.5b_tp4_pp1_4node_bf16_O2_100steps:
  stage: gpt3_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 5b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 4
    PP_SIZE: 1
    TIME_LIMIT: "4:00:00"
    AMP_STYLE: O2

train.gpt3.5b_tp8_pp1_4node_bf16_O2_100steps:
  stage: gpt3_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 5b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 8
    PP_SIZE: 1
    TIME_LIMIT: "4:00:00"
    AMP_STYLE: O2

train.gpt3.5b_tp2_pp2_4node_bf16_O2_100steps:
  stage: gpt3_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 5b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 2
    PP_SIZE: 2
    TIME_LIMIT: "4:00:00"
    AMP_STYLE: O2

train.gpt3.5b_tp4_pp2_4node_bf16_O2_100steps:
  stage: gpt3_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 5b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 4
    PP_SIZE: 2
    TIME_LIMIT: "4:00:00"
    AMP_STYLE: O2

train.gpt3.5b_tp8_pp2_4node_bf16_O2_100steps:
  stage: gpt3_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 5b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 8
    PP_SIZE: 2
    TIME_LIMIT: "4:00:00"
    AMP_STYLE: O2P

train.gpt3.5b_tp2_pp4_4node_bf16_O2_100steps:
  stage: gpt3_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 5b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 2
    PP_SIZE: 4
    TIME_LIMIT: "4:00:00"
    AMP_STYLE: O2

train.gpt3.5b_tp4_pp4_4node_bf16_O2_100steps:
  stage: gpt3_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 5b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 4
    PP_SIZE: 4
    TIME_LIMIT: "4:00:00"
    AMP_STYLE: O2

train.gpt3.5b_tp8_pp4_4node_bf16_O2_100steps:
  stage: gpt3_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 5b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 8
    PP_SIZE: 4
    TIME_LIMIT: "4:00:00"
    AMP_STYLE: O2

## T5
# O1
train.t5.3b_tp2_pp1_1node_fp16_O1_100steps:
  stage: t5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 1
    PRECISION: 16
    TP_SIZE: 2
    PP_SIZE: 1
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O1


train.t5.3b_tp4_pp1_1node_fp16_O1_100steps:
  stage: t5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 1
    PRECISION: 16
    TP_SIZE: 4
    PP_SIZE: 1
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O1

train.t5.3b_tp8_pp1_1node_fp16_O1_100steps:
  stage: t5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 1
    PRECISION: 16
    TP_SIZE: 8
    PP_SIZE: 1
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O1

train.t5.3b_tp2_pp1_1node_bf16_O1_100steps:
  stage: t5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 1
    PRECISION: bf16
    TP_SIZE: 2
    PP_SIZE: 1
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O1

train.t5.3b_tp4_pp1_1node_bf16_O1_100steps:
  stage: t5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 1
    PRECISION: bf16
    TP_SIZE: 4
    PP_SIZE: 1
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O1

train.t5.3b_tp8_pp1_1node_bf16_O1_100steps:
  stage: t5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 1
    PRECISION: bf16
    TP_SIZE: 8
    PP_SIZE: 1
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O1

train.t5.3b_tp2_pp1_4node_fp16_O1_100steps:
  stage: t5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 4
    PRECISION: 16
    TP_SIZE: 2
    PP_SIZE: 1
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O1

train.t5.3b_tp4_pp1_4node_fp16_O1_100steps:
  stage: t5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 4
    PRECISION: 16
    TP_SIZE: 4
    PP_SIZE: 1
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O1

train.t5.3b_tp8_pp1_4node_fp16_O1_100steps:
  stage: t5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 4
    PRECISION: 16
    TP_SIZE: 8
    PP_SIZE: 1
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O1

train.t5.3b_tp2_pp1_4node_bf16_O1_100steps:
  stage: t5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 2
    PP_SIZE: 1
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O1

train.t5.3b_tp4_pp1_4node_bf16_O1_100steps:
  stage: t5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 4
    PP_SIZE: 1
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O1

train.t5.3b_tp8_pp1_4node_bf16_O1_100steps:
  stage: t5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 8
    PP_SIZE: 1
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O1

train.t5.3b_tp2_pp2_4node_bf16_O1_100steps:
  stage: t5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 2
    PP_SIZE: 2
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O1

train.t5.3b_tp4_pp2_4node_bf16_O1_100steps:
  stage: t5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 4
    PP_SIZE: 2
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O1

train.t5.3b_tp8_pp2_4node_bf16_O1_100steps:
  stage: t5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 8
    PP_SIZE: 2
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O1

train.t5.3b_tp2_pp4_4node_bf16_O1_100steps:
  stage: t5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 2
    PP_SIZE: 4
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O1

train.t5.3b_tp4_pp4_4node_bf16_O1_100steps:
  stage: t5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 4
    PP_SIZE: 4
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O1

train.t5.3b_tp8_pp4_4node_bf16_O1_100steps:
  stage: t5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 8
    PP_SIZE: 4
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O1

# O2
train.t5.3b_tp2_pp1_1node_bf16_O2_100steps:
  stage: t5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 1
    PRECISION: bf16
    TP_SIZE: 2
    PP_SIZE: 1
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O2

train.t5.3b_tp4_pp1_1node_bf16_O2_100steps:
  stage: t5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 1
    PRECISION: bf16
    TP_SIZE: 4
    PP_SIZE: 1
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O2

train.t5.3b_tp8_pp1_1node_bf16_O2_100steps:
  stage: t5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 1
    PRECISION: bf16
    TP_SIZE: 8
    PP_SIZE: 1
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O2

train.t5.3b_tp2_pp1_4node_bf16_O2_100steps:
  stage: t5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 2
    PP_SIZE: 1
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O2

train.t5.3b_tp4_pp1_4node_bf16_O2_100steps:
  stage: t5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 1
    PRECISION: bf16
    TP_SIZE: 4
    PP_SIZE: 1
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O2

train.t5.3b_tp8_pp1_4node_bf16_O2_100steps:
  stage: t5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 8
    PP_SIZE: 1
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O2

train.t5.3b_tp2_pp2_4node_bf16_O2_100steps:
  stage: t5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 2
    PP_SIZE: 2
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O2

train.t5.3b_tp4_pp2_4node_bf16_O2_100steps:
  stage: t5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 4
    PP_SIZE: 2
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O2

train.t5.3b_tp8_pp2_4node_bf16_O2_100steps:
  stage: t5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 8
    PP_SIZE: 2
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O2

train.t5.3b_tp2_pp4_4node_bf16_O2_100steps:
  stage: t5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 2
    PP_SIZE: 4
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O2

train.t5.3b_tp4_pp4_4node_bf16_O2_100steps:
  stage: t5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 4
    PP_SIZE: 4
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O2

train.t5.3b_tp8_pp4_4node_bf16_O2_100steps:
  stage: t5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 8
    PP_SIZE: 4
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O2

## mT5
# O1
train.mt5.3b_tp2_pp1_1node_fp16_O1_100steps:
  stage: mt5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 1
    PRECISION: 16
    TP_SIZE: 2
    PP_SIZE: 1
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O1

train.mt5.3b_tp4_pp1_1node_fp16_O1_100steps:
  stage: mt5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 1
    PRECISION: 16
    TP_SIZE: 4
    PP_SIZE: 1
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O1

train.mt5.3b_tp8_pp1_1node_fp16_O1_100steps:
  stage: mt5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 1
    PRECISION: 16
    TP_SIZE: 8
    PP_SIZE: 1
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O1

train.mt5.3b_tp2_pp1_1node_bf16_O1_100steps:
  stage: mt5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 1
    PRECISION: bf16
    TP_SIZE: 2
    PP_SIZE: 1
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O1

train.mt5.3b_tp4_pp1_1node_bf16_O1_100steps:
  stage: mt5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 1
    PRECISION: bf16
    TP_SIZE: 4
    PP_SIZE: 1
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O1

train.mt5.3b_tp8_pp1_1node_bf16_O1_100steps:
  stage: mt5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 1
    PRECISION: bf16
    TP_SIZE: 8
    PP_SIZE: 1
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O1

train.mt5.3b_tp2_pp1_4node_fp16_O1_100steps:
  stage: mt5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 4
    PRECISION: 16
    TP_SIZE: 2
    PP_SIZE: 1
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O1

train.mt5.3b_tp4_pp1_4node_fp16_O1_100steps:
  stage: mt5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 4
    PRECISION: 16
    TP_SIZE: 4
    PP_SIZE: 1
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O1

train.mt5.3b_tp8_pp1_4node_fp16_O1_100steps:
  stage: mt5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 4
    PRECISION: 16
    TP_SIZE: 8
    PP_SIZE: 1
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O1

train.mt5.3b_tp2_pp1_4node_bf16_O1_100steps:
  stage: mt5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 2
    PP_SIZE: 1
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O1

train.mt5.3b_tp4_pp1_4node_bf16_O1_100steps:
  stage: mt5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 4
    PRECISION: 16
    TP_SIZE: 4
    PP_SIZE: 1
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O1

train.mt5.3b_tp8_pp1_4node_bf16_O1_100steps:
  stage: mt5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 8
    PP_SIZE: 1
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O1

train.mt5.3b_tp2_pp2_4node_bf16_O1_100steps:
  stage: mt5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 2
    PP_SIZE: 2
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O1

train.mt5.3b_tp4_pp2_4node_bf16_O1_100steps:
  stage: mt5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 4
    PP_SIZE: 2
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O1

train.mt5.3b_tp8_pp2_4node_bf16_O1_100steps:
  stage: mt5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 8
    PP_SIZE: 2
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O1

train.mt5.3b_tp2_pp4_4node_bf16_O1_100steps:
  stage: mt5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 2
    PP_SIZE: 4
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O1

train.mt5.3b_tp4_pp4_4node_bf16_O1_100steps:
  stage: mt5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 4
    PP_SIZE: 4
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O1

train.mt5.3b_tp8_pp4_4node_bf16_O1_100steps:
  stage: mt5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 8
    PP_SIZE: 4
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O1

# O2
train.mt5.3b_tp2_pp1_1node_bf16_O2_100steps:
  stage: mt5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 1
    PRECISION: bf16
    TP_SIZE: 2
    PP_SIZE: 1
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O2

train.mt5.3b_tp4_pp1_1node_bf16_O2_100steps:
  stage: mt5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 1
    PRECISION: bf16
    TP_SIZE: 4
    PP_SIZE: 1
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O2

train.mt5.3b_tp8_pp1_1node_bf16_O2_100steps:
  stage: mt5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 1
    PRECISION: bf16
    TP_SIZE: 8
    PP_SIZE: 1
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O2

train.mt5.3b_tp2_pp1_4node_bf16_O2_100steps:
  stage: mt5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 2
    PP_SIZE: 1
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O2

train.mt5.3b_tp4_pp1_4node_bf16_O2_100steps:
  stage: mt5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 4
    PP_SIZE: 1
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O2

train.mt5.3b_tp8_pp1_4node_bf16_O2_100steps:
  stage: mt5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 8
    PP_SIZE: 1
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O2

train.mt5.3b_tp2_pp2_4node_bf16_O2_100steps:
  stage: mt5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 2
    PP_SIZE: 2
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O2

train.mt5.3b_tp4_pp2_4node_bf16_O2_100steps:
  stage: mt5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 4
    PP_SIZE: 2
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O2

train.mt5.3b_tp8_pp2_4node_bf16_O2_100steps:
  stage: mt5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 8
    PP_SIZE: 2
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O2

train.mt5.3b_tp2_pp4_4node_bf16_O2_100steps:
  stage: mt5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 2
    PP_SIZE: 4
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O2

train.mt5.3b_tp4_pp4_4node_bf16_O2_100steps:
  stage: mt5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 4
    PP_SIZE: 4
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O2

train.mt5.3b_tp8_pp4_4node_bf16_O2_100steps:
  stage: mt5_configs
  <<: *bignlp-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 3b
    NUM_NODES: 4
    PRECISION: bf16
    TP_SIZE: 8
    PP_SIZE: 4
    TIME_LIMIT: "3:00:00"
    AMP_STYLE: O2

cleanup.selene:
  tags: *LUNA_TAGS
  stage: cleanup
  variables:
    <<: [*VARS, *LUNA_VARS]
  script:
    - rm -rf ${CI_BUILDS_DIR}/${SLURM_ACCOUNT}/big_nlp/bignlp_ci/*
    - echo "Finished cleaning everything in Selene"
  allow_failure: true
  rules:
    - when: manual

# Info on how to use the new CI system (https://docs.google.com/document/d/18uNblUvMvnN51Fda0itqsZzEXm9SJKQ-AcRMw6mGdCg/edit)

variables: &VARS
  BIGNLP_REGISTRY: "gitlab-master.nvidia.com:5005/dl/joc/bignlp-scripts"
  BIGNLP_BASE_IMAGE: "nvcr.io/nvidian/bignlp-training:22.08.01-py"
  SELENE_BIGNLP_CI_PATH: "/lustre/fsw/joc/big_nlp/bignlp_ci"
  NGC_BIGNLP_CI_PATH: "bignlp_ci" # mount at /mount/results
  CONTAINER_NVCR_BASE: nvcr.io/nvidian/bignlp-ci:training
  TESTS_TO_RUN_ON_MERGE_REQ: L0  # Can specify levels, ci job names etc as a space seperated list to run during merge request
  TESTS_TO_RUN_ON_THIS_COMMIT: L0 # Can specify levels, ci job names etc as a space seperated list to run during this commit
  TEST_REGEX_ON_THIS_COMMIT: NONE #https://github.com/google/re2/wiki/Syntax (Can define regex as in this spec) e.g /.*gpt3.*/


stages:
  - build
  - release_perf
  - test
  - cleanup


################
# JOB Templates
################

test:unit_tests:
  tags:
    - V100
  stage: test
  script:
    - pip install -r requirements.txt
    - pip install pytest
    - pip install requests-mock
    - export PATH="/home/gitlab-runner/.local/bin:$PATH"
    - pytest tests/unit_tests
  rules:
    - when: always

before_script:
  - umask 0007
  - export DOCKERFILE=./Dockerfile
  - export FROM_IMAGE_NAME=$BIGNLP_BASE_IMAGE
  - export IMAGE_NAME=bignlp_ci
  - export BUILD_IMAGE_NAME=${BIGNLP_REGISTRY}/${IMAGE_NAME}:pipe.${CI_PIPELINE_ID}
  - export BUILD_IMAGE_NAME_SRUN="${BUILD_IMAGE_NAME/:5005\//#}"
  - export NVCR_IMAGE_NAME=${CONTAINER_NVCR_BASE}.pipe.${CI_PIPELINE_ID}

.build: &build_template
  stage: build
  script:
    - docker login -u gitlab-ci-token -p $CI_JOB_TOKEN "${CI_REGISTRY}"
    - export DOCKER_REGISTRY="${CI_REGISTRY/:5005/}"
    - docker login -u gitlab-ci-token -p $CI_JOB_TOKEN "${DOCKER_REGISTRY}"
    - docker login -u "\$oauthtoken" -p $NGC_CLI_API_KEY nvcr.io
    # Define bignlp build image
    - set -x
    - ls
    - env
    - export FROM_IMAGE_ARG="--build-arg FROM_IMAGE_NAME=${FROM_IMAGE_NAME}"
    - docker build -t ${BUILD_IMAGE_NAME} ${FROM_IMAGE_ARG} .
    - docker push ${BUILD_IMAGE_NAME}
    # Push to NGC
    - docker tag ${BUILD_IMAGE_NAME} ${NVCR_IMAGE_NAME}
    - docker push ${NVCR_IMAGE_NAME}
  allow_failure: false
  tags:
    - vm-builder

.LUNA: &LUNA
  variables: &LUNA_VARS
    SLURM_PARTITION: "luna"
    SLURM_NIGHTLY_PARTITION: "luna"
    SLURM_ACCOUNT: "joc"
    CLUSTER:       "selene"
    PYXIS_LITE:    "1"
    ENROOT_MOUNT_HOME: "n"
    GIT_CLONE_PATH: $CI_BUILDS_DIR/$SLURM_ACCOUNT/big_nlp/bignlp_ci/$CI_PIPELINE_ID/$CI_JOB_ID/$CI_PROJECT_NAME # THIS DOES NOT HAVE QUOTES FOR A REASON
    EXCLUDE_NODES: ""
    GPU_ARCH:      "A100"
  tags: &LUNA_TAGS
    - selene_ssh

.NGC: &NGC
  variables: &NGC_VARS
    NGC_CLI_FORMAT_TYPE: "json"
    NGC_CLI_ACE: "nv-eagle"
    NGC_CLI_ORG: "nv-eagle-debug"
    NGC_CLI_TEAM: "no-team"
    NGC_CI_RESULT_WORKSPACE: "yuya-bignlp-ci"
  tags: &NGC_TAGS
    - ngc

.bignlp-NGC-test-LAUNCHER: &bignlp-NGC-test-LAUNCHER
  tags: *NGC_TAGS
  stage: test
  script:
    - set -x
    - pwd
    - export PIPELINE_DIR="${NGC_BIGNLP_CI_PATH}/${CI_PIPELINE_ID}"
    - export NGC_PROJECT_DIR="${PIPELINE_DIR}/${CI_JOB_ID}/${CI_PROJECT_NAME}" # GIT_CLONE_PATH on NGC
    - export BASE_RESULTS_DIR="${PIPELINE_DIR}/results"
    - export RUN_NAME=${RUN_TASK}_${RUN_MODEL}_${RUN_JOB_NAME}
    - export RESULTS_DIR=${BASE_RESULTS_DIR}/${RUN_NAME}
    - env
    - ngc workspace upload --source . --destination ${NGC_PROJECT_DIR} ${NGC_CI_RESULT_WORKSPACE}
    - bash tests/ci_tests/ngc/scripts/${RUN_TASK}/${RUN_MODEL}/${RUN_JOB_NAME}.sh | tee job_${CI_JOB_ID}.json
    - NGC_ID=$(cat job_${CI_JOB_ID}.json | jq -r ".id")
    - cd tests/ci_tests/utils
    - chmod 777 ./* -R
    - bash ngcjobwait.sh ${NGC_ID} 0
    - ngc batch attach ${NGC_ID}
    - bash ngcjobwait.sh ${NGC_ID} 1
    - bash ngcjobcheck.sh ${NGC_ID}
    - echo "Finished pytest job"

build-BigNLP:
  <<: *build_template
  rules:
    - when: always

release_perf:
  stage: release_perf
  trigger:
    include: tests/ci_tests/selene/release_perf/gitlab-ci.yml

.bignlp-LUNA-test-LAUNCHER-refactored: &bignlp-LUNA-test-LAUNCHER-refactored
  tags: *LUNA_TAGS
  stage: test
  script: &bignlp-LUNA-test-LAUNCHER-SCRIPT
    - chmod 774 ./* -R
    - umask 0007
    - source /lustre/fsw/joc/big_nlp/nemo_gpt3/my_venv/bin/activate
    - set -x
    - export PIPELINE_DIR="${SELENE_BIGNLP_CI_PATH}/${CI_PIPELINE_ID}"
    - export BASE_RESULTS_DIR="${PIPELINE_DIR}/results"
    - export RUN_NAME=${RUN_STAGE}_${RUN_MODEL}_${RUN_MODEL_SIZE}_tp${TP_SIZE}_pp${PP_SIZE}
    - if [[ "$RUN_STAGE" = "train" ]]; then export RUN_NAME=${RUN_NAME}_${NUM_NODES}node_${MAX_STEPS}steps; fi
    - if [[ "$RUN_STAGE" = "prompt_learn" ]]; then export RUN_NAME=${RUN_NAME}_${NUM_NODES}node_${TEST_TASK}; fi
    - if [[ "$RUN_STAGE" = "finetune" ]]; then export RUN_NAME=${RUN_NAME}_${NUM_NODES}node_${MAX_STEPS}steps_${TEST_TASK}; fi
    - if [[ "$RUN_STAGE" = "eval" ]]; then export RUN_NAME=${RUN_NAME}_${TEST_TASK}; fi
    - if [[ "$RUN_STAGE" = "data_prep" ]]; then export RUN_NAME=${RUN_STAGE}_${RUN_MODEL}_${DATASET}; fi
    - export RESULTS_DIR=${BASE_RESULTS_DIR}/${RUN_NAME}
    - env
    - |
        if [[ "$RUN_STAGE" = "data_prep" ]]; then
          bash tests/ci_tests/selene/scripts/${RUN_STAGE}/${RUN_MODEL}/${RUN_STAGE}_${DATASET}.sh
        else
          bash tests/ci_tests/selene/scripts/${RUN_STAGE}/${RUN_MODEL}/${RUN_STAGE}_${RUN_MODEL}_model.sh
        fi
    # Wait for job to launch
    - sleep 10s # Without this, "sacct" in jobstate.sh does not always find the SLURM job.
    - export SLURM_JOBID=$(grep 'Submitted batch job' "${RESULTS_DIR}/launcher.log" | awk '{ print $4 }')
    - echo $SLURM_JOBID
    - export SLURM_OUTPUT=${RESULTS_DIR}/*${SLURM_JOBID}.out
    #export SLURM_OUTPUT=$(scontrol show job "${SLURM_JOBID}" | grep 'StdOut' | awk -F '=' '{ print $2 }')
    - cd tests/ci_tests/utils
    - chmod 777 ./* -R
    - bash jobwait.sh "${SLURM_JOBID}" & PID=$!
    - touch "${SLURM_OUTPUT}"
    - \[ ! -z ${SLURM_JOBID} \] && echo -e " --------------------------------------------------\n"
                "----------WAITING FOR SLURM JOB TO BEGIN-----------\n"
                "---------------------------------------------------\n"
                "$(scontrol show job=${SLURM_JOBID})\n"
                "---------------------------------------------------\n"
    # Gitlab logs collapsible section markers
    - echo -e "\e[0Ksection_end:`date +%s`:slurm_setup\r\e[0K"
    # Follow output of the job
    - tail --pid="${PID}" -f "${SLURM_OUTPUT}" # Stream job output until it finishes.
    - echo "Finished job with name ${RUN_NAME}"
    - cd ${GIT_CLONE_PATH}
    # Run metrics collection
    - python3 tests/ci_tests/utils/convert_ci_metric_to_json.py $RESULTS_DIR $RUN_STAGE $RUN_MODEL
    # If you wan to be able to automatically add test results file then set the CREATE_TEST_DATA flag to be true.
    # It will run the test on selene copy the results file and paste it in the bignlp-scripts repo and raise a PR.
    - |
        if [[ "$CREATE_TEST_DATA" = "True" ]]; then
          echo "Current working directory"
          echo $pwd
          cd ..
          mkdir test
          cd test
          git clone ssh://git@gitlab-master.nvidia.com:12051/dl/JoC/bignlp-scripts.git
          cd bignlp-scripts
          git checkout -b $CI_PIPELINE_ID
          GIT_BRANCHES=`git branch -r`
          if [[ "$GIT_BRANCHES" =~ .*"$CI_PIPELINE_ID".* ]]; then git pull origin $CI_PIPELINE_ID; fi
          cp $RESULTS_DIR/*.json tests/ci_tests/selene/pytest/$RUN_STAGE/${RUN_MODEL}_result_files/
          git add tests/ci_tests/selene/pytest/$RUN_STAGE/${RUN_MODEL}_result_files/*
          sed -i 's/TESTS_TO_RUN_ON_THIS_COMMIT: .*/TESTS_TO_RUN_ON_THIS_COMMIT: NONE/g' .gitlab-ci.yml
          git add .gitlab-ci.yml
          git commit -m "Adding the result file for $RUN_NAME"
          git push origin $CI_PIPELINE_ID
          echo "Created test file and pushed it"
          exit 0
        fi
    # Run Pytest
    - pip3 install pytest
    - TEST_FILE=test_${RUN_STAGE}_pipeline.py
    # Will come in here from gpt3 and prompt_gpt3
    - if [[ "$RUN_STAGE" = "eval" && "$RUN_MODEL" == *"gpt3"* ]]; then TEST_FILE="test_gpt3_eval_pipeline.py"; fi
    - pytest tests/ci_tests/selene/pytest/${RUN_STAGE}/${TEST_FILE}
    - echo "Finished pytest job"
  rules:
    - if: $TEST_LEVEL =~ $TESTS_TO_RUN_ON_THIS_COMMIT || $CI_JOB_NAME =~ $TESTS_TO_RUN_ON_THIS_COMMIT || $CI_JOB_NAME =~ $TEST_REGEX_ON_THIS_COMMIT
      when: always
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event" && $TEST_LEVEL =~ $TESTS_TO_RUN_ON_MERGE_REQ'
      when: always
  allow_failure: false

train.gpt3.126m_tp1_pp1_1node_100steps:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: train
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 126m
    TP_SIZE: 1
    PP_SIZE: 1
    NUM_NODES: 1
    MAX_STEPS: 100
    TIME_LIMIT: "20:00"
    TEST_LEVEL: L0
  needs:
    - build-BigNLP

train.gpt3.126m_tp2_pp1_1node_100steps:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: train
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 126m
    TP_SIZE: 2
    PP_SIZE: 1
    NUM_NODES: 1
    MAX_STEPS: 100
    TIME_LIMIT: "20:00"
    TEST_LEVEL: L1
  needs:
    - build-BigNLP

train.gpt3.126m_tp1_pp2_2node_100steps:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: train
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 126m
    TP_SIZE: 1
    PP_SIZE: 2
    NUM_NODES: 2
    MAX_STEPS: 100
    TIME_LIMIT: "20:00"
    TEST_LEVEL: L1
  needs:
    - build-BigNLP

train.gpt3.126m_tp2_pp2_2node_100steps:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: train
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 126m
    TP_SIZE: 2
    PP_SIZE: 2
    NUM_NODES: 2
    MAX_STEPS: 100
    TIME_LIMIT: "20:00"
    TEST_LEVEL: L0
  needs:
    - build-BigNLP

train.gpt3.126m_tp4_pp4_4node_100steps:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: train
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 126m
    TP_SIZE: 4
    PP_SIZE: 4
    NUM_NODES: 4
    MAX_STEPS: 100
    TIME_LIMIT: "20:00"
    TEST_LEVEL: L1
  needs:
    - build-BigNLP

train.gpt3.5b_tp2_pp1_1node_100steps:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: train
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 5b
    TP_SIZE: 2
    PP_SIZE: 1
    NUM_NODES: 1
    MAX_STEPS: 100
    TIME_LIMIT: "4:00:00"
    TEST_LEVEL: L1
  needs:
    - build-BigNLP

train.gpt3.5b_tp2_pp2_2node_100steps:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: train
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 5b
    TP_SIZE: 2
    PP_SIZE: 2
    NUM_NODES: 2
    MAX_STEPS: 100
    TIME_LIMIT: "4:00:00"
    TEST_LEVEL: L1
  needs:
    - build-BigNLP

train.gpt3.20b_tp8_pp2_4node_100steps:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: train
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 20b
    TP_SIZE: 8
    PP_SIZE: 2
    NUM_NODES: 4
    MAX_STEPS: 100
    TIME_LIMIT: "3:00:00"
    TEST_LEVEL: L2
  needs:
    - build-BigNLP

train.gpt3.40b_tp8_pp4_8node_100steps:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: train
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 40b
    TP_SIZE: 8
    PP_SIZE: 4
    NUM_NODES: 8
    MAX_STEPS: 100
    TIME_LIMIT: "4:00:00"
    TEST_LEVEL: L2
  needs:
    - build-BigNLP

train.gpt3.175b_tp8_pp16_16node_50steps:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: train
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 175b
    TP_SIZE: 8
    PP_SIZE: 16
    NUM_NODES: 16
    MAX_STEPS: 50
    TIME_LIMIT: "4:00:00"
    TEST_LEVEL: L2
  needs:
    - build-BigNLP

train.t5.220m_tp1_pp1_1node_100steps:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: train
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 220m
    TP_SIZE: 1
    PP_SIZE: 1
    NUM_NODES: 1
    MAX_STEPS: 100
    TIME_LIMIT: "20:00"
    TEST_LEVEL: L0
  needs:
    - build-BigNLP

train.t5.220m_tp2_pp1_1node_100steps:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: train
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 220m
    TP_SIZE: 2
    PP_SIZE: 1
    NUM_NODES: 1
    MAX_STEPS: 100
    TIME_LIMIT: "20:00"
    TEST_LEVEL: L1
  needs:
    - build-BigNLP

train.t5.220m_tp1_pp2_2node_100steps:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: train
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 220m
    TP_SIZE: 1
    PP_SIZE: 2
    NUM_NODES: 2
    MAX_STEPS: 100
    TIME_LIMIT: "20:00"
    TEST_LEVEL: L1
  needs:
    - build-BigNLP

train.t5.220m_tp2_pp2_2node_100steps:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: train
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 220m
    TP_SIZE: 2
    PP_SIZE: 2
    NUM_NODES: 2
    MAX_STEPS: 100
    TIME_LIMIT: "20:00"
    TEST_LEVEL: L1
  needs:
    - build-BigNLP


train.mt5.170m_tp1_pp1_1node_100steps:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: train
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 170m
    TP_SIZE: 1
    PP_SIZE: 1
    NUM_NODES: 1
    MAX_STEPS: 100
    TIME_LIMIT: "20:00"
    TEST_LEVEL: L0
  needs:
    - build-BigNLP

train.mt5.170m_tp2_pp1_1node_100steps:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: train
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 170m
    TP_SIZE: 2
    PP_SIZE: 1
    NUM_NODES: 1
    MAX_STEPS: 100
    TIME_LIMIT: "20:00"
    TEST_LEVEL: L1
  needs:
    - build-BigNLP

train.mt5.170m_tp1_pp2_2node_100steps:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: train
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 170m
    TP_SIZE: 1
    PP_SIZE: 2
    NUM_NODES: 2
    MAX_STEPS: 100
    TIME_LIMIT: "20:00"
    TEST_LEVEL: L1
  needs:
    - build-BigNLP

train.mt5.170m_tp2_pp2_2node_100steps:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: train
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 170m
    TP_SIZE: 2
    PP_SIZE: 2
    NUM_NODES: 2
    MAX_STEPS: 100
    TIME_LIMIT: "20:00"
    TEST_LEVEL: L1
  needs:
    - build-BigNLP

eval.gpt3.126m_tp1_pp1_lambada:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: eval
    TEST_TASK: lambada
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 126m
    TIME_LIMIT: "30:00"
    TP_SIZE: 1
    PP_SIZE: 1
    TRAIN_JOB_NAME: train_gpt3_126m_tp1_pp1_1node_100steps
    TEST_LEVEL: L0
  needs:
    - train.gpt3.126m_tp1_pp1_1node_100steps


eval.gpt3.126m_tp2_pp1_lambada:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: eval
    TEST_TASK: lambada
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 126m
    TIME_LIMIT: "30:00"
    TP_SIZE: 2
    PP_SIZE: 1
    TRAIN_JOB_NAME: train_gpt3_126m_tp2_pp1_1node_100steps
    TEST_LEVEL: L1
  needs:
    - train.gpt3.126m_tp2_pp1_1node_100steps


eval.gpt3.126m_tp1_pp2_lambada:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: eval
    TEST_TASK: lambada
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 126m
    TIME_LIMIT: "30:00"
    TP_SIZE: 1
    PP_SIZE: 2
    TRAIN_JOB_NAME: train_gpt3_126m_tp1_pp2_2node_100steps
    TEST_LEVEL: L1
  needs:
    - train.gpt3.126m_tp1_pp2_2node_100steps


eval.gpt3.126m_tp2_pp2_lambada:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: eval
    TEST_TASK: lambada
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 126m
    TIME_LIMIT: "30:00"
    TP_SIZE: 2
    PP_SIZE: 2
    TRAIN_JOB_NAME: train_gpt3_126m_tp2_pp2_2node_100steps
    TEST_LEVEL: L0
  needs:
    - train.gpt3.126m_tp2_pp2_2node_100steps


convert.gpt3.126m_tp1_pp1:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: convert
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 126m
    UPSTREAM_RUN_NAME: train_gpt3_126m_tp1_pp1_1node_100steps
    TP_SIZE: 1
    PP_SIZE: 1
    TIME_LIMIT: "30:00"
    TEST_LEVEL: L0
  needs:
    - train.gpt3.126m_tp1_pp1_1node_100steps


convert.gpt3.126m_tp2_pp1:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: convert
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 126m
    UPSTREAM_RUN_NAME: train_gpt3_126m_tp2_pp1_1node_100steps
    TP_SIZE: 2
    PP_SIZE: 1
    TIME_LIMIT: "30:00"
    TEST_LEVEL: L1
  needs:
    - train.gpt3.126m_tp2_pp1_1node_100steps


convert.gpt3.126m_tp1_pp2:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: convert
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 126m
    UPSTREAM_RUN_NAME: train_gpt3_126m_tp1_pp2_2node_100steps
    TP_SIZE: 1
    PP_SIZE: 2
    TIME_LIMIT: "30:00"
    TEST_LEVEL: L1
  needs:
    - train.gpt3.126m_tp1_pp2_2node_100steps


convert.gpt3.126m_tp2_pp2:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: convert
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 126m
    UPSTREAM_RUN_NAME: train_gpt3_126m_tp2_pp2_2node_100steps
    TP_SIZE: 2
    PP_SIZE: 2
    TIME_LIMIT: "30:00"
    TEST_LEVEL: L0
  needs:
    - train.gpt3.126m_tp2_pp2_2node_100steps

convert.t5.220m_tp1_pp1:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: convert
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 220m
    UPSTREAM_RUN_NAME: train_t5_220m_tp1_pp1_1node_100steps
    TP_SIZE: 1
    PP_SIZE: 1
    TIME_LIMIT: "30:00"
    TEST_LEVEL: L0
  needs:
    - train.t5.220m_tp1_pp1_1node_100steps

convert.t5.220m_tp2_pp2:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: convert
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 220m
    UPSTREAM_RUN_NAME: train_t5_220m_tp2_pp2_2node_100steps
    TP_SIZE: 2
    PP_SIZE: 2
    TIME_LIMIT: "30:00"
    TEST_LEVEL: L1
  needs:
    - train.t5.220m_tp2_pp2_2node_100steps

finetune.t5.220m_tp1_pp1_1node_100steps_mnli:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: finetune
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 220m
    TP_SIZE: 1
    PP_SIZE: 1
    MAX_STEPS: 100
    NUM_NODES: 1
    TIME_LIMIT: "40:00"
    TEST_LEVEL: L0
    TEST_TASK: mnli
  needs:
    - convert.t5.220m_tp1_pp1

finetune.t5.220m_tp2_pp2_2node_100steps_mnli:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: finetune
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 220m
    TP_SIZE: 2
    PP_SIZE: 2
    MAX_STEPS: 100
    NUM_NODES: 2
    TIME_LIMIT: "40:00"
    TEST_LEVEL: L1
    TEST_TASK: mnli
  needs:
    - convert.t5.220m_tp2_pp2

eval.t5.220m_tp1_pp1:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: eval
    TEST_TASK: mnli_matched
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 220m
    TIME_LIMIT: "1:00:00"
    TP_SIZE: 1
    PP_SIZE: 1
    NUM_NODES: 1
    FINETUNE_JOB_DIR: finetune_t5_220m_tp1_pp1_1node_100steps_mnli
    TEST_LEVEL: L0
  needs:
    - finetune.t5.220m_tp1_pp1_1node_100steps_mnli

eval.t5.220m_tp2_pp2:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: eval
    TEST_TASK: mnli_matched
    RUN_MODEL: t5
    RUN_MODEL_SIZE: 220m
    TIME_LIMIT: "1:00:00"
    TP_SIZE: 2
    PP_SIZE: 2
    NUM_NODES: 2
    FINETUNE_JOB_DIR: finetune_t5_220m_tp2_pp2_2node_100steps_mnli
    MICRO_BATCH_SIZE: 8
    TEST_LEVEL: L1
  needs:
    - finetune.t5.220m_tp2_pp2_2node_100steps_mnli

convert.mt5.170m_tp1_pp1:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: convert
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 170m
    UPSTREAM_RUN_NAME: train_mt5_170m_tp1_pp1_1node_100steps
    TP_SIZE: 1
    PP_SIZE: 1
    TIME_LIMIT: "30:00"
    TEST_LEVEL: L0
  needs:
    - train.mt5.170m_tp1_pp1_1node_100steps

convert.mt5.170m_tp2_pp2:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: convert
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 170m
    UPSTREAM_RUN_NAME: train_mt5_170m_tp2_pp2_2node_100steps
    TP_SIZE: 2
    PP_SIZE: 2
    TIME_LIMIT: "30:00"
    TEST_LEVEL: L1
  needs:
    - train.mt5.170m_tp2_pp2_2node_100steps

finetune.mt5.170m_tp1_pp1_1node_100steps_xnli:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: finetune
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 170m
    TP_SIZE: 1
    PP_SIZE: 1
    MAX_STEPS: 100
    NUM_NODES: 1
    TIME_LIMIT: "30:00"
    TEST_LEVEL: L0
    TEST_TASK: xnli
  needs:
    - convert.mt5.170m_tp1_pp1

finetune.mt5.170m_tp2_pp2_2node_100steps_xnli:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: finetune
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 170m
    TP_SIZE: 2
    PP_SIZE: 2
    MAX_STEPS: 100
    NUM_NODES: 2
    TIME_LIMIT: "40:00"
    TEST_LEVEL: L1
    TEST_TASK: xnli
  needs:
    - convert.mt5.170m_tp2_pp2

eval.mt5.170m_tp1_pp1:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: eval
    TEST_TASK: xnli
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 170m
    TIME_LIMIT: "1:00:00"
    TP_SIZE: 1
    PP_SIZE: 1
    NUM_NODES: 1
    FINETUNE_JOB_DIR: finetune_mt5_170m_tp1_pp1_1node_100steps_xnli
    TEST_LEVEL: L0
  needs:
    - finetune.mt5.170m_tp1_pp1_1node_100steps_xnli

eval.mt5.170m_tp2_pp2:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: eval
    TEST_TASK: xnli
    RUN_MODEL: mt5
    RUN_MODEL_SIZE: 170m
    TIME_LIMIT: "1:00:00"
    TP_SIZE: 2
    PP_SIZE: 2
    NUM_NODES: 2
    FINETUNE_JOB_DIR: finetune_mt5_170m_tp2_pp2_2node_100steps_xnli
    MICRO_BATCH_SIZE: 8
    TEST_LEVEL: L1
  needs:
    - finetune.mt5.170m_tp2_pp2_2node_100steps_xnli

prompt_learn.gpt3.126m_tp1_pp1_1node_100steps_squad:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: prompt_learn
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 126m
    TEST_TASK: squad
    TP_SIZE: 1
    PP_SIZE: 1
    NUM_NODES: 1
    MAX_STEPS: 100
    TIME_LIMIT: "30:00"
    TEST_LEVEL: L0
  needs:
    - convert.gpt3.126m_tp1_pp1

prompt_learn.gpt3.126m_tp1_pp1_1node_squad_real:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: prompt_learn
    RUN_MODEL: gpt3
    RUN_MODEL_SIZE: 126m
    TEST_TASK: squad_real
    LANGUAGE_MODEL_PATH: /lustre/fsw/joc/big_nlp/bignlp_ci_resources/checkpoints/gpt3_126m_bf16_O2_tp1_pp1.nemo
    CONTAINER_MOUNTS: "container_mounts=[/lustre/fsw/joc/big_nlp/bignlp_ci_resources:/lustre/fsw/joc/big_nlp/bignlp_ci_resources,/lustre/fsw/joc/yuya/bignlp/bignlp-scripts_gpt3/data:/lustre/fsw/joc/yuya/bignlp/bignlp-scripts_gpt3/data]"
    TP_SIZE: 1
    PP_SIZE: 1
    NUM_NODES: 1
    TIME_LIMIT: "01:00:00"
    TEST_LEVEL: L1
  needs:
    - build-BigNLP

eval.gpt3.prompt_126m_tp1_pp1_squad:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: eval
    RUN_MODEL: prompt_gpt3
    RUN_MODEL_SIZE: 126m
    TEST_TASK: squad
    PROMPT_LEARN_MODEL_DIR: prompt_learn_gpt3_126m_tp1_pp1_1node_squad
    TIME_LIMIT: "30:00"
    TP_SIZE: 1
    PP_SIZE: 1
    TEST_LEVEL: L0
  needs:
    - prompt_learn.gpt3.126m_tp1_pp1_1node_100steps_squad

eval.gpt3.prompt_126m_tp1_pp1_squad_real:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: eval
    RUN_MODEL: prompt_gpt3
    RUN_MODEL_SIZE: 126m
    TEST_TASK: squad_real
    PROMPT_LEARN_MODEL_DIR: prompt_learn_gpt3_126m_tp1_pp1_1node_squad_real
    TIME_LIMIT: "30:00"
    TP_SIZE: 1
    PP_SIZE: 1
    TEST_LEVEL: L1
  needs:
    - prompt_learn.gpt3.126m_tp1_pp1_1node_squad_real

data_prep.gpt3.the_pile_prepare_shard00:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: data_prep
    RUN_MODEL: gpt3
    DATASET: pile
    TEST_LEVEL: L2
  needs:
    - build-BigNLP


data_prep.t5.the_pile_prepare_shard00:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: data_prep
    RUN_MODEL: t5
    DATASET: pile
    TEST_LEVEL: L2
  needs:
    - build-BigNLP


data_prep.mt5.mc4_prepare_lang_mt:
  <<: *bignlp-LUNA-test-LAUNCHER-refactored
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_STAGE: data_prep
    RUN_MODEL: mt5
    DATASET: mc4
    TEST_LEVEL: L2
  needs:
    - build-BigNLP

train.gpt3.ngc_126m_tp1_pp1_2node_100steps:
  <<: *bignlp-NGC-test-LAUNCHER
  variables:
    <<: [*VARS, *NGC_VARS]
    RUN_TASK: train
    RUN_MODEL: gpt3
    RUN_JOB_NAME: 126m_tp1_pp1_2node_100steps
  rules:
    - when: never
  needs:
    - build-BigNLP

cleanup.selene:
  tags: *LUNA_TAGS
  stage: cleanup
  variables:
    <<: [*VARS, *LUNA_VARS]
  script:
    - rm -rf ${CI_BUILDS_DIR}/${SLURM_ACCOUNT}/big_nlp/bignlp_ci/*
    - echo "Finished cleaning everything in Selene"
  allow_failure: true
  rules:
    - when: manual

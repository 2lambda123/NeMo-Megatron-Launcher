name: megatron_inference_benchmark-${inference.run.model_train_name}

run:
  name: ${.task_name}_${.model_train_name}
  time_limit: "04:00:00"
  dependency: "singleton"
  convert_name: convert_nemo
  model_train_name: gptnext
  convert_dir: ${base_results_dir}/${inference.run.model_train_name}/${inference.run.convert_name}
  task_name: "squad"
  results_dir: ${base_results_dir}/${.model_train_name}/inference_${.task_name}

trainer:
  devices: ${divide_ceil:${evaluation.model_parallel_size}, ${.num_nodes}}
  num_nodes: ${divide_ceil:${evaluation.model_parallel_size}, 8}
  accelerator: gpu
  precision: bf16
  logger: False # logger provided by exp_manager
  enable_checkpointing: False
  use_distributed_sampler: False
  log_every_n_steps: 10

inference:
  greedy: True # Whether or not to use sampling ; use greedy decoding otherwise
  top_k: 1  # The number of highest probability vocabulary tokens to keep for top-k-filtering.
  top_p: 0 # If set to float < 1, only the most probable tokens with probabilities that add up to top_p or higher are kept for generation.
  temperature: 0 # sampling temperature
  add_BOS: True # add the bos token at the begining of the prompt
  tokens_to_generate: 30 # The minimum length of the sequence to be generated.
  all_probs: False  # whether return the log prob for all the tokens in vocab
  repetition_penalty: 1.2  # The parameter for repetition penalty. 1.0 means no penalty.
  min_tokens_to_generate: 0  # The minimum length of the sequence to be generated.
  compute_logprob: False  # a flag used to compute logprob of all the input text, a very special case of running inference, default False

tensor_model_parallel_size: 1
pipeline_model_parallel_size: 1
pipeline_model_parallel_split_rank: ${divide_floor:${.pipeline_model_parallel_size}, 2} # used for encoder and decoder model
model_parallel_size: ${multiply:${.tensor_model_parallel_size}, ${.pipeline_model_parallel_size}}

nemo_checkpoint: null
model_type: gptnext
num_gpus: 1
max_input_len: 250
max_batch_size: 10
num_runs: 50


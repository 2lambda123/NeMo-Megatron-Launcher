train_settings:
  model_size: 20b
  trainer:
    num_nodes: 64
    devices: 8
    preciosion: bf16
    max_steps: 75000
    val_check_interval: 2000
  model:
    tensor_parallel_size: 4
    pipeline_parallel_size: 1
    global_batch_size: 2048
    micro_batch_size: 4
    rampup_batch_size: null
    act_ckpt_layers: 0
    transformer_engine: True
    optim:
      name: distributed_fused_adam
      lr: 1.4e-4
      min_lr: 1.4e-5
        
